

## Commands to run:


## Start up interactive session

## For full:
## qrsh -q gpu.q@@dgx -l num_proc=8,mem_free=500G,h_rt=8:00:00,gpu=8

## For small:
## qrsh -q gpu.q@@dgx -l num_proc=8,mem_free=40G,h_rt=8:00:00,gpu=2

## Switch cuda to correct one
## source /exp/rkriz/packages/switch-cuda/switch-cuda.sh 11.3

## Start up API
## metaseq-api-local

## Error 1 
## RuntimeError: You must set the variables in metaseq.service.constants to launch the API.
## Answer: we need the model weights, plus dict.txt and things in projects/OPT/assets copied to model file
## Also need to change hard coded model path in metaseq/service/constants.py

## Error 2:
## File "/exp/rkriz/venv_opt/lib/python3.8/site-packages/tokenizers/implementations/byte_level_bpe.py", line 82, in from_file
##     vocab, merges = BPE.read_file(vocab_filename, merges_filename)
## Exception: Error while reading vocab & merges files: No such file or directory (os error 2)
## Fixed by copy/pasting things in projects/OPT/assets to model directory

## Error 3:
##   File "/exp/rkriz/packages/metaseq/metaseq/checkpoint_utils.py", line 339, in _is_checkpoint_sharded
##   size_ratio = max(sizes) / min(sizes)
## ValueError: max() arg is an empty sequence
## Problem: We're not actually loading in a model right now


## Initial step: we need to consolidate the 992 shards into 8 files, by running this:
## bash metaseq/scripts/reshard_mp_launch_no_slurm.sh /exp/rkriz/models/OPT/175B/checkpoint_last /exp/rkriz/models/OPT/175B_reshard/ 8 1
## This crashed an entire node on the COE grid...so it may not be 


#### HUGGINGFACE (FOR SMALLER MODELS) ####

import os
from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM

weights_path = '/exp/rkriz/packages/opt_metaseq_125m/model/'
checkpoint = 'facebook/opt-125m'

# If the folder contains a checkpoint that isn't sharded, it needs to point to the state dict directly
# otherwise point to the directory containing the shard
files = os.listdir(weights_path)
weights_path = os.path.join(weights_path, 'pytorch_model.bin') if 'pytorch_model.bin' in files else weights_path

config = AutoConfig.from_pretrained(weights_path)

# Initializes an empty shell with the model. This is instant and does not take any RAM.
with init_empty_weights():
    model = AutoModelForCausalLM.from_config(config)

# Initialize the model under the previous context manager breaks the tied weights.
model.tie_weights()

# Infer device map automatically
device_map = infer_auto_device_map(model.model, no_split_module_classes=["OPTDecoderLayer"], dtype='float16')

if any([k == 'disk' for k in device_map.values()]):
    offload_folder = 'offload_folder'
else:
    offload_folder = None

if '30b' in checkpoint:
    # Set a few layers to use the disk manually to ensure enough RAM for the 30B checkpoint.
    device_map['decoder.layers.23'] = 'disk'
    device_map['decoder.layers.24'] = 'disk'
    device_map['decoder.layers.25'] = 'disk'
    device_map['decoder.layers.26'] = 'disk'
    device_map['decoder.layers.27'] = 'disk'

device_map

load_checkpoint_and_dispatch(
    model.model, 
    weights_path, 
    device_map=device_map, 
    offload_folder=offload_folder, 
    dtype='float16', 
    offload_state_dict=True
)
model.tie_weights()




### Attempt 2
from transformers import pipeline

generator = pipeline('text-generation', model="facebook/opt-125m")
generator("Tag the following for entities:\nJohn lived in New York.")

generator("I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \"Unknown\".\n\nQ: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: Unknown\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.\n\nQ: How many squigs are in a bonk?\nA: Unknown\n\nQ: Where is the Valley of Kings?")

## from transformers import pipeline, set_seed
## set_seed(32)
## generator = pipeline('text-generation', model="facebook/opt-125m", do_sample=True)
## generator("Hello, I'm am conscious and")
## [{'generated_text': "Hello, I'm am conscious and active observer!! HmmregorCLASSIFIEDドラゴン覚醒ドラゴンドラゴン覚醒覚醒ドラゴン"}]
```
