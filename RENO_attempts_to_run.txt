

## Commands to run:


## Start up interactive session

## For full:
## qrsh -q gpu.q@@dgx -l num_proc=8,mem_free=300G,h_rt=8:00:00,gpu=8

## For small:
## qrsh -q gpu.q@@dgx -l num_proc=1,mem_free=40G,h_rt=1:00:00,gpu=1

## Switch cuda to correct one
## source /exp/rkriz/packages/switch-cuda/switch-cuda.sh 11.3

## Start up API
## metaseq-api-local

## Error 1 
## RuntimeError: You must set the variables in metaseq.service.constants to launch the API.
## Answer: we need the model weights, plus dict.txt and things in projects/OPT/assets copied to model file
## Also need to change hard coded model path in metaseq/service/constants.py

## Error 2:
## File "/exp/rkriz/venv_opt/lib/python3.8/site-packages/tokenizers/implementations/byte_level_bpe.py", line 82, in from_file
##     vocab, merges = BPE.read_file(vocab_filename, merges_filename)
## Exception: Error while reading vocab & merges files: No such file or directory (os error 2)
## Fixed by copy/pasting things in projects/OPT/assets to model directory

## Error 3:
##   File "/exp/rkriz/packages/metaseq/metaseq/checkpoint_utils.py", line 339, in _is_checkpoint_sharded
##   size_ratio = max(sizes) / min(sizes)
## ValueError: max() arg is an empty sequence
## Problem: We're not actually loading in a model right now


## Initial step: we need to consolidate the 992 shards into 8 files, by running this:
## bash metaseq/scripts/reshard_mp_launch_no_slurm.sh /exp/rkriz/models/OPT/175B/checkpoint_last /exp/rkriz/models/OPT/175B_reshard/ 8 1
## This crashed an entire node on the COE grid...also this may not be feasible

#### OPT-66B
## This should be loadable in memory, and does not need to be re-sharded, as it already comes in 8 parts

## ERROR 3 fix:
## Fixed model paths and suffix so that script correctly reads in all 8 checkpoints
## This got us through reading from disk! It took 11:15 to load

## ERROR 4:
## AssertionError: intra_layer_model parallel group is not initialized
## Github issue: https://github.com/facebookresearch/metaseq/issues/26
## Possible solution: we need to set model-parallel to 8
metaseq-api-local --model-parallel 8
## Going down to 125M results in the same error (setting model-parallel and world-size to 2)

## 6/23 Where are we at?
## we can load in a model from disk, but we can't then actually use it because of Error 4
## Also, we don't need to run the API, we need to just pass in data
## Huggingface can provide solutions for up to 30B, but it's unclear if this can be loaded without the model being sharded (hugginface combines the shards together into a single model before using).

## 6/24 update: this is not true!! snapshot_download downloads the 30B model in 8 parts, so if we can download it we should be able to use it within huggingface
## Potential issue: snapshot_download sends things to cache, which has much smaller storage
## Answer: Add the cache_dir argument to change it!


#### HUGGINGFACE (FOR SMALLER MODELS) ####

import os
from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM

weights_path = '/exp/rkriz/packages/opt_metaseq_125m/model/'
checkpoint = 'facebook/opt-30b'

# If the folder contains a checkpoint that isn't sharded, it needs to point to the state dict directly
# otherwise point to the directory containing the shard
files = os.listdir(weights_path)
weights_path = os.path.join(weights_path, 'pytorch_model.bin') if 'pytorch_model.bin' in files else weights_path

config = AutoConfig.from_pretrained(weights_path)

# Initializes an empty shell with the model. This is instant and does not take any RAM.
with init_empty_weights():
    model = AutoModelForCausalLM.from_config(config)

# Initialize the model under the previous context manager breaks the tied weights.
model.tie_weights()

# Infer device map automatically
device_map = infer_auto_device_map(model.model, no_split_module_classes=["OPTDecoderLayer"], dtype='float16')

if any([k == 'disk' for k in device_map.values()]):
    offload_folder = 'offload_folder'
else:
    offload_folder = None

if '30b' in checkpoint:
    # Set a few layers to use the disk manually to ensure enough RAM for the 30B checkpoint.
    device_map['decoder.layers.23'] = 'disk'
    device_map['decoder.layers.24'] = 'disk'
    device_map['decoder.layers.25'] = 'disk'
    device_map['decoder.layers.26'] = 'disk'
    device_map['decoder.layers.27'] = 'disk'

device_map

load_checkpoint_and_dispatch(
    model.model, 
    weights_path, 
    device_map=device_map, 
    offload_folder=offload_folder, 
    dtype='float16', 
    offload_state_dict=True
)
model.tie_weights()




### Attempt 2
from transformers import pipeline

generator = pipeline('text-generation', model="facebook/opt-125m")
generator("Tag the following for entities:\nJohn lived in New York.")

generator("I am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with \"Unknown\".\n\nQ: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: Unknown\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.\n\nQ: How many squigs are in a bonk?\nA: Unknown\n\nQ: Where is the Valley of Kings?")

## from transformers import pipeline, set_seed
## set_seed(32)
## generator = pipeline('text-generation', model="facebook/opt-125m", do_sample=True)
## generator("Hello, I'm am conscious and")
## [{'generated_text': "Hello, I'm am conscious and active observer!! HmmregorCLASSIFIEDドラゴン覚醒ドラゴンドラゴン覚醒覚醒ドラゴン"}]
```







#### transformers with accelerate

## cache_dir

from huggingface_hub import snapshot_download
checkpoint = 'facebook/opt-125m'
weights_path = snapshot_download(checkpoint, cache_dir="/exp/rkriz/models/OPT/125M_hf/")

import os
files = os.listdir(weights_path)
weights_path = os.path.join(weights_path, 'pytorch_model.bin') if 'pytorch_model.bin' in files else weights_path

from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map, load_checkpoint_and_dispatch, load_checkpoint_in_model
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM

config = AutoConfig.from_pretrained(checkpoint)

# Initializes an empty shell with the model. This is instant and does not take any RAM.
with init_empty_weights():
    model = AutoModelForCausalLM.from_config(config)

# Initialize the model under the previous context manager breaks the tied weights.
model.tie_weights()

# Infer device map automatically
device_map = infer_auto_device_map(model.model, no_split_module_classes=["OPTDecoderLayer"], dtype='float16')

if any([k == 'disk' for k in device_map.values()]):
    offload_folder = 'offload_folder'
else:
    offload_folder = None

if '125m' in checkpoint:
    device_map['decoder.embed_tokens'] = 0
    device_map['decoder.embed_positions'] = 0

if '30b' in checkpoint:
    # Set a few layers to use the disk manually to ensure enough RAM for the 30B checkpoint.
    device_map['decoder.layers.23'] = 'disk'
    device_map['decoder.layers.24'] = 'disk'
    device_map['decoder.layers.25'] = 'disk'
    device_map['decoder.layers.26'] = 'disk'
    device_map['decoder.layers.27'] = 'disk'

device_map

load_checkpoint_and_dispatch(
    model.model, 
    weights_path, 
    device_map=device_map, 
    offload_folder=offload_folder, 
    dtype='float16', 
    offload_state_dict=True
)
model.tie_weights()


tokenizer = AutoTokenizer.from_pretrained('facebook/opt-30b')
inputs = tokenizer("Hugging Face is pushing the convention that a unicorn with two horns becomes a llama.", return_tensors="pt")

output = model.generate(inputs["input_ids"].to(0), max_length=10, do_sample=True)

print(tokenizer.decode(output[0].tolist()))
